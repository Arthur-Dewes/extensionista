{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fc7ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:18.543880Z",
     "start_time": "2025-05-21T17:30:13.462305Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be36272-a118-4874-b7c8-3b0f4bd6c1d9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Leitura e Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94b7d0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:18.568818Z",
     "start_time": "2025-05-21T17:30:18.559884Z"
    }
   },
   "outputs": [],
   "source": [
    "# EI: ensino infantil\n",
    "# EF: ensino fundamental\n",
    "# EM: ensino m√©dio\n",
    "# EJA: educa√ß√£o de jovens e adultos (f: fundamental; m: m√©dio)\n",
    "# EP: educa√ß√£o profissional\n",
    "# ES: educa√ß√£o especial\n",
    "# gn: grupo 1, grupo 2, grupo 3, grupo 4, grupo 5\n",
    "# nvln: n√≠vel 1, n√≠vel 2, n√≠vel 3, n√≠vel 4, n√≠vel 5\n",
    "\n",
    "# adequacao da formacao do docente\n",
    "afd_cols    = ['ano', 'unidade_geografica', 'localizacao', 'dependencia_administrativa',\n",
    "            'EI_g1', 'EI_g2', 'EI_g3', 'EI_g4', 'EI_g5',\n",
    "            'EF_total_g1', 'EF_total_g2', 'EF_total_g3', 'EF_total_g4', 'EF_total_g5',\n",
    "            'EF_anos_iniciais_g1', 'EF_anos_iniciais_g2', 'EF_anos_iniciais_g3', 'EF_anos_iniciais_g4', 'EF_anos_iniciais_g5',\n",
    "            'EF_anos_finais_g1', 'EF_anos_finais_g2', 'EF_anos_finais_g3', 'EF_anos_finais_g4', 'EF_anos_finais_g5',\n",
    "            'EM_g1', 'EM_g2', 'EM_g3', 'EM_g4', 'EM_g5',\n",
    "            'EJAf_g1', 'EJAf_g2', 'EJAf_g3', 'EJAf_g4', 'EJAf_g5',\n",
    "            'EJAm_g1', 'EJAm_g2', 'EJAm_g3', 'EJAm_g4', 'EJAm_g5']\n",
    "\n",
    "# percentual dos docentes com curso superior\n",
    "dsu_cols = ['ano', 'unidade_geografica', 'localizacao', 'dependencia_administrativa', \n",
    "           'EI_total', 'EI_creche', 'EI_pre_escola',\n",
    "           'EF_total', 'EF_anos_iniciais', 'EF_anos_finais',\n",
    "           'EM', 'EP', 'EJA', 'ES']\n",
    "\n",
    "# regualridade do corpo docente\n",
    "ird_cols = ['ano', 'unidade_geografica', 'localizacao', 'dependencia_administrativa', \n",
    "           'baixa_regularidade(0-|2)', 'media_baixa(2-|3)', 'media_alta(3-|4)', 'alta(4-|5)']\n",
    "\n",
    "# esforco docente\n",
    "ied_cols = ['ano', 'unidade_geografica', 'localizacao', 'dependencia_administrativa',\n",
    "        'EF_total_nvl1', 'EF_total_nvl2','EF_total_nvl3', 'EF_total_nvl4', 'EF_total_nvl5',\n",
    "        'EF_total_nvl6','EF_anos_iniciais_nvl1', 'EF_anos_iniciais_nvl2','EF_anos_iniciais_nvl3', 'EF_anos_iniciais_nvl4', 'EF_anos_iniciais_nvl5', 'EF_anos_iniciais_nvl6','EF_anos_finais_nvl1', 'EF_anos_finais_nvl2','EF_anos_finais_nvl3', 'EF_anos_finais_nvl4', 'EF_anos_finais_nvl5', 'EF_anos_finais_nvl6','EM_nvl1', 'EM_nvl2','EM_nvl3', 'EM_nvl4', 'EM_nvl5', 'EM_nvl6']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6f10aa",
   "metadata": {},
   "source": [
    "## 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5209131",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:20.919788Z",
     "start_time": "2025-05-21T17:30:18.573816Z"
    }
   },
   "outputs": [],
   "source": [
    "df_AFD_2018 = pd.read_excel('extensionista/dados2018/AFD_BRASIL_REGIOES_UFS_2018.xlsx', names=afd_cols, header=None, skiprows=11, nrows=587, engine='openpyxl')\n",
    "df_DSU_2018 = pd.read_excel('extensionista/dados2018/DSU_BRASIL_REGIOES_UFS_2018_ATUALIZADO.xlsx', names=dsu_cols, header=None, skiprows=10, nrows=587, engine='openpyxl')\n",
    "df_IRD_2018 = pd.read_excel('extensionista/dados2018/IRD_BRASIL_REGIOES_UFS_2018.xlsx', names=ird_cols, header=None, skiprows=10, nrows=586, engine='openpyxl')\n",
    "df_IED_2018 = pd.read_excel('extensionista/dados2018/IED_BRASIL_REGIOES_UFS_2018.xlsx', names = ied_cols,header=None,skiprows=11, nrows=586, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58fcdb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:22.332330Z",
     "start_time": "2025-05-21T17:30:22.314889Z"
    }
   },
   "outputs": [],
   "source": [
    "float_cols = df_AFD_2018.columns.to_list()[4:]\n",
    "df_AFD_2018[float_cols] = df_AFD_2018[float_cols].apply(pd.to_numeric, errors='coerce')\n",
    "float_cols = df_IED_2018.columns.to_list()[4:]\n",
    "df_IED_2018[float_cols] = df_IED_2018[float_cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d1b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_AFD_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b19259-96e1-4a6f-b748-3ec320e0c94c",
   "metadata": {},
   "source": [
    "Filtrar as linhas para pegar apenas as regioes e excluir colunas que nao iremos utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ac3cf-f7b3-4a3f-90d5-70a4db7b6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "regioes_AFD_2018 = df_AFD_2018.drop(['EJAf_g1', 'EJAf_g2', 'EJAf_g3', 'EJAf_g4', 'EJAf_g5',\n",
    "           'EJAm_g1', 'EJAm_g2', 'EJAm_g3', 'EJAm_g4', 'EJAm_g5'], axis=1).loc[:107]\n",
    "\n",
    "regioes_DSU_2018 = df_DSU_2018.drop(['EP', 'EJA', 'ES'],axis=1).loc[:107]\n",
    "regioes_IRD_2018 = df_IRD_2018.loc[:107]\n",
    "regioes_IED_2018 = df_IED_2018.loc[:107]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7825a",
   "metadata": {},
   "source": [
    "## 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abdcdbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:23.042984Z",
     "start_time": "2025-05-21T17:30:22.392035Z"
    }
   },
   "outputs": [],
   "source": [
    "df_AFD_2019 = pd.read_excel('extensionista/dados2019/AFD_BRASIL_REGIOES_UFS_2019.xlsx', names=afd_cols, header=None, skiprows=11, nrows=587, engine='openpyxl')\n",
    "df_DSU_2019 = pd.read_excel('extensionista/dados2019/DSU_BRASIL_REGIOES_UFS_2019.xlsx', names=dsu_cols, header=None, skiprows=10, nrows=587, engine='openpyxl')\n",
    "df_IRD_2019 = pd.read_excel('extensionista/dados2019/IRD_BRASIL_REGIOES_UFS_2019.xlsx', names=ird_cols, header=None, skiprows=10, nrows=586, engine='openpyxl')\n",
    "df_IED_2019 = pd.read_excel('extensionista/dados2019/IED_BRASIL_REGIOES_UFS_2019.xlsx', names = ied_cols,header=None,skiprows=11, nrows=586, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5038eb15da7ee724",
   "metadata": {},
   "source": [
    "Filtrar as linhas para pegar apenas as regioes e excluir colunas que nao iremos utilizar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb14a2e41941af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:23.092578Z",
     "start_time": "2025-05-21T17:30:23.086497Z"
    }
   },
   "outputs": [],
   "source": [
    "regioes_AFD_2019 = df_AFD_2019.drop(['EJAf_g1', 'EJAf_g2', 'EJAf_g3', 'EJAf_g4', 'EJAf_g5',\n",
    "           'EJAm_g1', 'EJAm_g2', 'EJAm_g3', 'EJAm_g4', 'EJAm_g5'], axis=1).loc[:107]\n",
    "\n",
    "regioes_DSU_2019 = df_DSU_2019.drop(['EP', 'EJA', 'ES'],axis=1).loc[:107]\n",
    "regioes_IRD_2019 = df_IRD_2019.loc[:107]\n",
    "regioes_IED_2019 = df_IED_2019.loc[:107]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100f60d1",
   "metadata": {},
   "source": [
    "## 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54390d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:23.781481Z",
     "start_time": "2025-05-21T17:30:23.146588Z"
    }
   },
   "outputs": [],
   "source": [
    "df_AFD_2022 = pd.read_excel('extensionista/dados2022/AFD_BRASIL_REGIOES_UFS_2022.xlsx', names=afd_cols, header=None, skiprows=11, nrows=587, engine='openpyxl')\n",
    "df_DSU_2022 = pd.read_excel('extensionista/dados2022/DSU_BRASIL_REGIOES_UFS_2022.xlsx', names=dsu_cols, header=None, skiprows=10, nrows=587, engine='openpyxl')\n",
    "df_IRD_2022 = pd.read_excel('extensionista/dados2022/IRD_BRASIL_REGIOES_UFS_2022.xlsx', names=ird_cols, header=None, skiprows=10, nrows=586, engine='openpyxl')\n",
    "df_IED_2022 = pd.read_excel('extensionista/dados2022/IED_BRASIL_REGIOES_UFS_2022.xlsx', names = ied_cols,header=None,skiprows=11, nrows=586, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0c30b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:23.864497Z",
     "start_time": "2025-05-21T17:30:23.845493Z"
    }
   },
   "outputs": [],
   "source": [
    "float_cols = df_AFD_2022.columns.to_list()[4:]\n",
    "df_AFD_2022[float_cols] = df_AFD_2022[float_cols].apply(pd.to_numeric, errors='coerce')\n",
    "float_cols = df_DSU_2022.columns.to_list()[4:]\n",
    "df_DSU_2022[float_cols] = df_DSU_2022[float_cols].apply(pd.to_numeric, errors='coerce')\n",
    "float_cols = df_IED_2022.columns.to_list()[4:]\n",
    "df_IED_2022[float_cols] = df_IED_2022[float_cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a475b1e68d8b11cd",
   "metadata": {},
   "source": [
    "Filtrar as linhas para pegar apenas as regioes e excluir colunas que nao iremos utilizar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887d2a87a987515",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:23.939567Z",
     "start_time": "2025-05-21T17:30:23.930807Z"
    }
   },
   "outputs": [],
   "source": [
    "regioes_AFD_2022 = df_AFD_2022.drop(['EJAf_g1', 'EJAf_g2', 'EJAf_g3', 'EJAf_g4', 'EJAf_g5',\n",
    "           'EJAm_g1', 'EJAm_g2', 'EJAm_g3', 'EJAm_g4', 'EJAm_g5'], axis=1).loc[:107]\n",
    "\n",
    "regioes_DSU_2022 = df_DSU_2022.drop(['EP', 'EJA', 'ES'],axis=1).loc[:107]\n",
    "regioes_IRD_2022 = df_IRD_2022.loc[:107]\n",
    "regioes_IED_2022 = df_IED_2022.loc[:107]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c12aa5a",
   "metadata": {},
   "source": [
    "## 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd6dd83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:24.669332Z",
     "start_time": "2025-05-21T17:30:24.094478Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_AFD_2023 = pd.read_excel('extensionista/dados2023/AFD_BRASIL_REGIOES_UFS_2023.xlsx', names=afd_cols, header=None, skiprows=11, nrows=587, engine='openpyxl')\n",
    "df_DSU_2023 = pd.read_excel('extensionista/dados2023/DSU_BRASIL_REGIOES_UFS_2023.xlsx', names=dsu_cols, header=None, skiprows=10, nrows=587, engine='openpyxl')\n",
    "df_IRD_2023 = pd.read_excel('extensionista/dados2023/IRD_BRASIL_REGIOES_UFS_2023.xlsx', names=ird_cols, header=None, skiprows=10, nrows=586, engine='openpyxl')\n",
    "df_IED_2023 = pd.read_excel('extensionista/dados2023/IED_BRASIL_REGIOES_UFS_2023.xlsx', names = ied_cols,header=None,skiprows=11, nrows=586, engine='openpyxl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe194b71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:24.693150Z",
     "start_time": "2025-05-21T17:30:24.674324Z"
    }
   },
   "outputs": [],
   "source": [
    "float_cols = df_AFD_2023.columns.to_list()[4:]\n",
    "df_AFD_2023[float_cols] = df_AFD_2023[float_cols].apply(pd.to_numeric, errors='coerce')\n",
    "float_cols = df_DSU_2023.columns.to_list()[4:]\n",
    "df_DSU_2023[float_cols] = df_DSU_2023[float_cols].apply(pd.to_numeric, errors='coerce')\n",
    "float_cols = df_IED_2023.columns.to_list()[4:]\n",
    "df_IED_2023[float_cols] = df_IED_2023[float_cols].apply(pd.to_numeric, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f99e6c97e3e3cf",
   "metadata": {},
   "source": [
    "Filtrar as linhas para pegar apenas as regioes e excluir colunas que nao iremos utilizar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d26d5eaabb4a12",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:24.791368Z",
     "start_time": "2025-05-21T17:30:24.781879Z"
    }
   },
   "outputs": [],
   "source": [
    "regioes_AFD_2023 = df_AFD_2023.drop(['EJAf_g1', 'EJAf_g2', 'EJAf_g3', 'EJAf_g4', 'EJAf_g5',\n",
    "           'EJAm_g1', 'EJAm_g2', 'EJAm_g3', 'EJAm_g4', 'EJAm_g5'], axis=1).loc[:107]\n",
    "\n",
    "regioes_DSU_2023 = df_DSU_2023.drop(['EP', 'EJA', 'ES'],axis=1).loc[:107]\n",
    "regioes_IRD_2023 = df_IRD_2023.loc[:107]\n",
    "regioes_IED_2023 = df_IED_2023.loc[:107]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f04600c-a45f-4da1-87b9-e3ea4a4318df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Divis√£o entre regi√µes e ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d7641-4d33-43a8-a4a2-cc71378bd7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrar_regioes_por_ano(dfs_por_ano, regioes=[\"Sul\", \"Sudeste\", \"Norte\", \"Nordeste\", \"Centro-Oeste\", \"Brasil\"]):\n",
    "    resultado = {}\n",
    "\n",
    "    for ano, dfs in dfs_por_ano.items():  # ex: ano = 2018, dfs = {\"AFD\": df, \"DSU\": df, ...}\n",
    "        for tipo, df in dfs.items():  # tipo = \"AFD\", \"DSU\", etc.\n",
    "            for regiao in regioes:\n",
    "                nome_var = f\"Regiao_{regiao.replace('-', '')}_{tipo}_{ano}\"\n",
    "                resultado[nome_var] = df[df[\"unidade_geografica\"] == regiao]\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "dfs_por_ano = {\n",
    "    2018: {\"AFD\": regioes_AFD_2018, \"DSU\": regioes_DSU_2018, \"IRD\": regioes_IRD_2018, \"IED\": regioes_IED_2018},\n",
    "    2019: {\"AFD\": regioes_AFD_2019, \"DSU\": regioes_DSU_2019, \"IRD\": regioes_IRD_2019, \"IED\": regioes_IED_2019},\n",
    "    2022: {\"AFD\": regioes_AFD_2022, \"DSU\": regioes_DSU_2022, \"IRD\": regioes_IRD_2022, \"IED\": regioes_IED_2022},\n",
    "    2023: {\"AFD\": regioes_AFD_2023, \"DSU\": regioes_DSU_2023, \"IRD\": regioes_IRD_2023, \"IED\": regioes_IED_2023},\n",
    "}\n",
    "\n",
    "regioes_filtradas = filtrar_regioes_por_ano(dfs_por_ano)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99dafc74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Verifica√ß√£o para saber se todos possuem o mesmo shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c815e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T17:30:24.898162Z",
     "start_time": "2025-05-21T17:30:24.891160Z"
    }
   },
   "outputs": [],
   "source": [
    "bases = ['AFD', 'DSU', 'IED', 'IRD']\n",
    "anos = [2018, 2019, 2022, 2023]\n",
    "\n",
    "# Comparar os dataframes com prefixo 'df_'\n",
    "print(\"\\nCompara√ß√µes para 'df_':\")\n",
    "dif_dfs = []\n",
    "for base in bases:\n",
    "    shape_2018 = globals()[f\"df_{base}_2018\"].shape\n",
    "\n",
    "    for ano in anos:\n",
    "        nome_df = f\"df_{base}_{ano}\"\n",
    "        df = globals()[nome_df]\n",
    "        same_shape = df.shape == shape_2018\n",
    "        if not same_shape:\n",
    "            print(f\"{nome_df} ‚Üí Shape diferente: {df.shape}, esperado: {shape_2018}\")\n",
    "            dif_dfs.append(nome_df)\n",
    "if not dif_dfs:\n",
    "    print(\"Todos os DataFrames 'df_' t√™m o mesmo shape que os de 2018.\")\n",
    "else:\n",
    "    print(\"Alguns DataFrames 'df_' t√™m shape diferente dos de 2018.\")\n",
    "\n",
    "print()\n",
    "\n",
    "# Comparar os dataframes com prefixo 'regioes_'\n",
    "print(\"Compara√ß√µes para 'regioes_':\")\n",
    "dif_regioes = []\n",
    "for base in bases:\n",
    "    shape_2018 = globals()[f\"regioes_{base}_2018\"].shape\n",
    "\n",
    "    for ano in anos:\n",
    "        nome_df = f\"regioes_{base}_{ano}\"\n",
    "        df = globals()[nome_df]\n",
    "        same_shape = df.shape == shape_2018\n",
    "        if not same_shape:\n",
    "            print(f\"{nome_df} ‚Üí Shape diferente: {df.shape}, esperado: {shape_2018}\")\n",
    "            dif_regioes.append(nome_df)\n",
    "\n",
    "if not dif_regioes:\n",
    "    print(\"Todos os DataFrames 'regioes_' t√™m o mesmo shape que os de 2018.\")\n",
    "else:\n",
    "    print(\"Alguns DataFrames 'regioes_' t√™m shape diferente dos de 2018.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34814a96-1aa1-4039-ae28-9f535e696795",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Estat√≠stica descritiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43cc6ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T07:15:01.296848Z",
     "start_time": "2025-05-21T07:15:01.291454Z"
    }
   },
   "outputs": [],
   "source": [
    "def media(col):\n",
    "    \"\"\"M√©dia de uma coluna (Series)\"\"\"\n",
    "    return col.mean()\n",
    "\n",
    "def desvio_padrao(col):\n",
    "    \"\"\"Desvio padr√£o (amostral) da coluna (Series)\"\"\"\n",
    "    return col.std()\n",
    "\n",
    "def coeficiente_variacao(col):\n",
    "    \"\"\"Coeficiente de varia√ß√£o (%). Se a m√©dia for zero, retorna inf\"\"\"\n",
    "    media_col = col.mean()\n",
    "    if media_col == 0:\n",
    "        return float('inf')\n",
    "    return (col.std() / media_col) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d3cc07-ffe1-4134-b518-bb5fbed70ac8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Correla√ß√£o entre vari√°veis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ce17b5f76fd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_correlacoes_completas(dfs_por_ano, regioes=['Sul', 'Sudeste', 'Norte', 'Nordeste', 'Centro-Oeste', 'Brasil']):\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    for ano, dfs in dfs_por_ano.items():\n",
    "        with pd.ExcelWriter(f'output/correlacoes_{ano}.xlsx', engine='openpyxl') as writer:\n",
    "            for regiao in regioes:\n",
    "                df_afn = dfs['AFD'].loc[lambda df: df['unidade_geografica'] == regiao]\n",
    "                df_dsu = dfs['DSU'].loc[lambda df: df['unidade_geografica'] == regiao]\n",
    "                df_ird = dfs['IRD'].loc[lambda df: df['unidade_geografica'] == regiao]\n",
    "                df_ied = dfs['IED'].loc[lambda df: df['unidade_geografica'] == regiao]\n",
    "\n",
    "                if df_afn.empty and df_dsu.empty and df_ird.empty and df_ied.empty:\n",
    "                    continue\n",
    "\n",
    "                def prepara(df, prefix):\n",
    "                    num = df.select_dtypes(include='number')\n",
    "                    num.columns = [f'{prefix}_{c}' for c in num.columns]\n",
    "                    return num.reset_index(drop=True)\n",
    "\n",
    "                a = prepara(df_afn, 'AFD')\n",
    "                b = prepara(df_dsu, 'DSU')\n",
    "                c = prepara(df_ird, 'IRD')\n",
    "                d = prepara(df_ied, 'IED')\n",
    "\n",
    "                all_vars = pd.concat([a, b, c, d], axis=1)\n",
    "\n",
    "                corr = all_vars.corr()\n",
    "\n",
    "                # save sheet\n",
    "                corr.to_excel(writer, sheet_name=regiao[:31])\n",
    "\n",
    "calcular_correlacoes_completas(dfs_por_ano)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ae5e26-896d-4244-b974-c2ecc42e4b40",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Aplica√ß√£o de estat√≠stica descritiva"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e6c50-1d95-4024-9d31-943ec22444ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_distribuicoes(dfs_por_ano,\n",
    "                           regioes=[\"Sul\", \"Sudeste\", \"Norte\", \"Nordeste\", \"Centro-Oeste\", \"Brasil\"]):\n",
    "    os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "    for ano, dfs in dfs_por_ano.items():\n",
    "\n",
    "        with pd.ExcelWriter(f'output/metricas_distribuicao_{ano}.xlsx', engine='openpyxl') as writer:\n",
    "            for regiao in regioes:\n",
    "                partes = []\n",
    "                for base, df in dfs.items():\n",
    "                    df_reg = df[df[\"unidade_geografica\"] == regiao]\n",
    "                    if df_reg.empty:\n",
    "                        continue\n",
    "                    num = df_reg.select_dtypes(include='number')\n",
    "                    num.columns = [f\"{base}_{c}\" for c in num.columns]\n",
    "                    partes.append(num)\n",
    "                \n",
    "                if not partes:\n",
    "                    continue\n",
    "\n",
    "                dados = pd.concat(partes, axis=0, ignore_index=True)\n",
    "\n",
    "                resumo = pd.DataFrame({\n",
    "                    'media':              dados.apply(media),\n",
    "                    'desvio_padrao':      dados.apply(desvio_padrao),\n",
    "                    'coef_variacao_%':    dados.apply(coeficiente_variacao),\n",
    "                })\n",
    "\n",
    "                resumo = resumo.round(2)\n",
    "\n",
    "                # save sheet\n",
    "                resumo.to_excel(writer, sheet_name=regiao[:31])\n",
    "\n",
    "calcular_distribuicoes(dfs_por_ano)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1f3514-533e-4c1f-bdf8-0155a641b50c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Plot das features de correla√ß√£o mais extrema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1fac4-001c-4127-a507-36a70122a7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pares_correlacao_alta(ano, regiao, corte, pasta='output'):\n",
    "    \n",
    "    arquivo = f'{pasta}/correlacoes_{ano}.xlsx'\n",
    "    corr = pd.read_excel(arquivo, sheet_name=regiao, index_col=0)\n",
    "    \n",
    "    mask = corr.abs() > corte\n",
    "    for idx in corr.index:\n",
    "        mask.at[idx, idx] = False\n",
    "\n",
    "    resultados = []\n",
    "    for i in range(len(corr.index)):\n",
    "        for j in range(i+1, len(corr.columns)):\n",
    "            if mask.iat[i, j]:\n",
    "                var_i = corr.index[i]\n",
    "                var_j = corr.columns[j]\n",
    "                val = corr.iat[i, j]\n",
    "                resultados.append((var_i, var_j, val))\n",
    "    \n",
    "    df_pares = pd.DataFrame(resultados, columns=['var1', 'var2', 'correlacao'])\n",
    "    \n",
    "    df_positivos = df_pares[df_pares['correlacao'] >= corte].reset_index(drop=True)\n",
    "    df_negativos = df_pares[df_pares['correlacao'] <= -corte].reset_index(drop=True)\n",
    "    \n",
    "    return df_positivos, df_negativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5378b9-2642-4a09-878d-d0cd51f85db8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for ano in anos:\n",
    "    df_pos, df_neg = pares_correlacao_alta(ano, 'Brasil', 0.7, pasta='output')\n",
    "    \n",
    "    top_pos = df_pos.nlargest(3, 'correlacao')\n",
    "    top_neg = df_neg.nsmallest(3, 'correlacao')\n",
    "    top3    = pd.concat([top_pos, top_neg], ignore_index=True)\n",
    "    \n",
    "    vars_pref = pd.unique(top3[['var1','var2']].values.ravel())\n",
    "    \n",
    "    blocos = []\n",
    "    for base, df_base in dfs_por_ano[ano].items():\n",
    "        df_b = df_base[df_base['unidade_geografica'] == 'Brasil']\n",
    "        num = df_b.select_dtypes(include='number')\n",
    "        # adiciona prefixo para corresponder a var1/var2\n",
    "        num.columns = [f\"{base}_{c}\" for c in num.columns]\n",
    "        blocos.append(num)\n",
    "    df_region = pd.concat(blocos, axis=1).reset_index(drop=True)\n",
    "    \n",
    "    for _, row in top3.iterrows():\n",
    "        v1, v2 = row['var1'], row['var2']\n",
    "        if v1 in df_region.columns and v2 in df_region.columns:\n",
    "            plt.figure()\n",
    "            plt.plot(df_region.index, df_region[v1], label=v1)\n",
    "            plt.plot(df_region.index, df_region[v2], label=v2)\n",
    "            plt.title(f'{ano} ‚Äì {v1} VS {v2}\\nCorrela√ß√£o = {row[\"correlacao\"]:.3f}')\n",
    "            plt.xlabel('Observa√ß√£o')\n",
    "            plt.ylabel('Valor')\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc1737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcular_variacao_percentual(df_ano1, df_ano2, base_nome):\n",
    "    \"\"\"\n",
    "    Calcula a varia√ß√£o percentual entre dois DataFrames do mesmo tipo de base\n",
    "    \"\"\"\n",
    "    # Garantir que ambos os DataFrames tenham as mesmas regi√µes\n",
    "    regioes_comuns = set(df_ano1['unidade_geografica']).intersection(\n",
    "        set(df_ano2['unidade_geografica'])\n",
    "    )\n",
    "    \n",
    "    # Filtrar apenas as regi√µes comuns\n",
    "    df1_filtrado = df_ano1[df_ano1['unidade_geografica'].isin(regioes_comuns)].copy()\n",
    "    df2_filtrado = df_ano2[df_ano2['unidade_geografica'].isin(regioes_comuns)].copy()\n",
    "    \n",
    "    # Resetar √≠ndices para garantir alinhamento\n",
    "    df1_filtrado = df1_filtrado.reset_index(drop=True)\n",
    "    df2_filtrado = df2_filtrado.reset_index(drop=True)\n",
    "    \n",
    "    # Criar DataFrame resultado com colunas b√°sicas\n",
    "    resultado = pd.DataFrame()\n",
    "    resultado['unidade_geografica'] = df1_filtrado['unidade_geografica']\n",
    "    resultado['localizacao'] = df1_filtrado['localizacao']\n",
    "    resultado['dependencia_administrativa'] = df1_filtrado['dependencia_administrativa']\n",
    "    \n",
    "    # Calcular varia√ß√£o percentual para colunas num√©ricas\n",
    "    colunas_numericas = df1_filtrado.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for coluna in colunas_numericas:\n",
    "        if coluna in df2_filtrado.columns:\n",
    "            # Evitar divis√£o por zero\n",
    "            denominador = df1_filtrado[coluna].replace(0, np.nan)\n",
    "            variacao = ((df2_filtrado[coluna] - df1_filtrado[coluna]) / denominador) * 100\n",
    "            resultado[f'{coluna}_var_%'] = variacao.round(2)\n",
    "    \n",
    "    return resultado\n",
    "\n",
    "def gerar_comparacoes_temporais(dfs_por_ano, comparacoes, regioes=['Sul', 'Sudeste', 'Norte', 'Nordeste', 'Centro-Oeste', 'Brasil']):\n",
    "    \"\"\"\n",
    "    Gera arquivos Excel com compara√ß√µes temporais para cada par de anos\n",
    "    \"\"\"\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    bases = ['AFD', 'DSU', 'IRD', 'IED']\n",
    "    \n",
    "    for (ano1, ano2), nome_arquivo in comparacoes.items():\n",
    "        \n",
    "        with pd.ExcelWriter(f'output/{nome_arquivo}', engine='openpyxl') as writer:\n",
    "            \n",
    "            for base in bases:\n",
    "                \n",
    "                # Obter DataFrames dos dois anos para a base atual\n",
    "                df_ano1 = dfs_por_ano[ano1][base]\n",
    "                df_ano2 = dfs_por_ano[ano2][base]\n",
    "                \n",
    "                # Calcular varia√ß√£o percentual\n",
    "                df_variacao = calcular_variacao_percentual(df_ano1, df_ano2, base)\n",
    "                \n",
    "                # Filtrar apenas as regi√µes de interesse (se necess√°rio)\n",
    "                df_variacao_filtrado = df_variacao[\n",
    "                    df_variacao['unidade_geografica'].isin(regioes)\n",
    "                ].copy()\n",
    "                \n",
    "                # Adicionar colunas com anos de refer√™ncia\n",
    "                df_variacao_filtrado.insert(0, 'ano_base', ano1)\n",
    "                df_variacao_filtrado.insert(1, 'ano_comparacao', ano2)\n",
    "                \n",
    "                # Salvar na sheet correspondente\n",
    "                df_variacao_filtrado.to_excel(writer, sheet_name=base, index=False)\n",
    "        \n",
    "\n",
    "def gerar_resumo_estatistico(dfs_por_ano, comparacoes):\n",
    "    \"\"\"\n",
    "    Gera um resumo estat√≠stico das varia√ß√µes para an√°lise r√°pida\n",
    "    \"\"\"\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    bases = ['AFD', 'DSU', 'IRD', 'IED']\n",
    "    \n",
    "    with pd.ExcelWriter('output/resumo_variacoes_estatisticas.xlsx', engine='openpyxl') as writer:\n",
    "        \n",
    "        for (ano1, ano2), _ in comparacoes.items():\n",
    "            resumos = []\n",
    "            \n",
    "            for base in bases:\n",
    "                df_ano1 = dfs_por_ano[ano1][base]\n",
    "                df_ano2 = dfs_por_ano[ano2][base]\n",
    "                \n",
    "                df_variacao = calcular_variacao_percentual(df_ano1, df_ano2, base)\n",
    "                \n",
    "                # Calcular estat√≠sticas das varia√ß√µes\n",
    "                colunas_var = [col for col in df_variacao.columns if col.endswith('_var_%')]\n",
    "                \n",
    "                for coluna in colunas_var:\n",
    "                    dados_limpos = df_variacao[coluna].dropna()\n",
    "                    if len(dados_limpos) > 0:\n",
    "                        resumos.append({\n",
    "                            'comparacao': f'{ano1}_vs_{ano2}',\n",
    "                            'base': base,\n",
    "                            'variavel': coluna,\n",
    "                            'media_variacao_%': dados_limpos.mean(),\n",
    "                            'mediana_variacao_%': dados_limpos.median(),\n",
    "                            'desvio_padrao_%': dados_limpos.std(),\n",
    "                            'min_variacao_%': dados_limpos.min(),\n",
    "                            'max_variacao_%': dados_limpos.max(),\n",
    "                            'qtd_observacoes': len(dados_limpos)\n",
    "                        })\n",
    "            \n",
    "            if resumos:\n",
    "                df_resumo = pd.DataFrame(resumos)\n",
    "                df_resumo = df_resumo.round(2)\n",
    "                df_resumo.to_excel(writer, sheet_name=f'{ano1}_vs_{ano2}', index=False)\n",
    "\n",
    "# Fun√ß√£o adicional para visualizar maiores varia√ß√µes\n",
    "def identificar_maiores_variacoes(arquivo_excel, base, top_n=10):\n",
    "    \"\"\"\n",
    "    Identifica as maiores varia√ß√µes (positivas e negativas) em uma base espec√≠fica\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(arquivo_excel, sheet_name=base)\n",
    "    \n",
    "    # Encontrar colunas de varia√ß√£o\n",
    "    colunas_var = [col for col in df.columns if col.endswith('_var_%')]\n",
    "    \n",
    "    maiores_variacoes = []\n",
    "    \n",
    "    for coluna in colunas_var:\n",
    "        dados_limpos = df[coluna].dropna()\n",
    "        if len(dados_limpos) > 0:\n",
    "            # Maiores aumentos\n",
    "            max_idx = dados_limpos.idxmax()\n",
    "            maiores_variacoes.append({\n",
    "                'tipo': 'maior_aumento',\n",
    "                'variavel': coluna,\n",
    "                'regiao': df.loc[max_idx, 'unidade_geografica'],\n",
    "                'localizacao': df.loc[max_idx, 'localizacao'],\n",
    "                'dependencia': df.loc[max_idx, 'dependencia_administrativa'],\n",
    "                'variacao_%': dados_limpos.max()\n",
    "            })\n",
    "            \n",
    "            # Maiores diminui√ß√µes\n",
    "            min_idx = dados_limpos.idxmin()\n",
    "            maiores_variacoes.append({\n",
    "                'tipo': 'maior_diminuicao',\n",
    "                'variavel': coluna,\n",
    "                'regiao': df.loc[min_idx, 'unidade_geografica'],\n",
    "                'localizacao': df.loc[min_idx, 'localizacao'],\n",
    "                'dependencia': df.loc[min_idx, 'dependencia_administrativa'],\n",
    "                'variacao_%': dados_limpos.min()\n",
    "            })\n",
    "    \n",
    "    df_resultado = pd.DataFrame(maiores_variacoes)\n",
    "    return df_resultado.sort_values('variacao_%', key=abs, ascending=False).head(top_n)\n",
    "\n",
    "# Fun√ß√£o para gerar relat√≥rio consolidado por regi√£o\n",
    "def gerar_relatorio_por_regiao():\n",
    "    \"\"\"\n",
    "    Gera um relat√≥rio consolidado mostrando como cada regi√£o evoluiu ao longo do tempo\n",
    "    \"\"\"\n",
    "    os.makedirs('output', exist_ok=True)\n",
    "    \n",
    "    regioes = ['Sul', 'Sudeste', 'Norte', 'Nordeste', 'Centro-Oeste', 'Brasil']\n",
    "    bases = ['AFD', 'DSU', 'IRD', 'IED']\n",
    "    \n",
    "    with pd.ExcelWriter('output/relatorio_evolucao_por_regiao.xlsx', engine='openpyxl') as writer:\n",
    "        \n",
    "        for regiao in regioes:\n",
    "            dados_regiao = []\n",
    "            \n",
    "            for (ano1, ano2), _ in comparacoes.items():\n",
    "                for base in bases:\n",
    "                    df_ano1 = dfs_por_ano[ano1][base]\n",
    "                    df_ano2 = dfs_por_ano[ano2][base]\n",
    "                    \n",
    "                    # Filtrar dados da regi√£o espec√≠fica\n",
    "                    dados_reg_ano1 = df_ano1[df_ano1['unidade_geografica'] == regiao]\n",
    "                    dados_reg_ano2 = df_ano2[df_ano2['unidade_geografica'] == regiao]\n",
    "                    \n",
    "                    if not dados_reg_ano1.empty and not dados_reg_ano2.empty:\n",
    "                        df_var = calcular_variacao_percentual(dados_reg_ano1, dados_reg_ano2, base)\n",
    "                        \n",
    "                        # Calcular m√©dia das varia√ß√µes para esta base e per√≠odo\n",
    "                        colunas_var = [col for col in df_var.columns if col.endswith('_var_%')]\n",
    "                        for coluna in colunas_var:\n",
    "                            media_var = df_var[coluna].mean()\n",
    "                            if not pd.isna(media_var):\n",
    "                                dados_regiao.append({\n",
    "                                    'periodo': f'{ano1}-{ano2}',\n",
    "                                    'base': base,\n",
    "                                    'indicador': coluna.replace('_var_%', ''),\n",
    "                                    'variacao_media_%': round(media_var, 2)\n",
    "                                })\n",
    "            \n",
    "            if dados_regiao:\n",
    "                df_regiao = pd.DataFrame(dados_regiao)\n",
    "                df_regiao.to_excel(writer, sheet_name=regiao, index=False)\n",
    "    \n",
    "    print(\"Relat√≥rio por regi√£o salvo em: relatorio_evolucao_por_regiao.xlsx\")\n",
    "\n",
    "# Fun√ß√£o para an√°lise de tend√™ncias\n",
    "def analisar_tendencias():\n",
    "    \"\"\"\n",
    "    Analisa tend√™ncias gerais nos dados educacionais\n",
    "    \"\"\"\n",
    "    \n",
    "    bases = ['AFD', 'DSU', 'IRD', 'IED']\n",
    "    \n",
    "    for base in bases:\n",
    "        \n",
    "        # Dados do Brasil para todos os anos\n",
    "        dados_brasil = []\n",
    "        for ano in [2018, 2019, 2022, 2023]:\n",
    "            df_base = dfs_por_ano[ano][base]\n",
    "            brasil_data = df_base[df_base['unidade_geografica'] == 'Brasil']\n",
    "            if not brasil_data.empty:\n",
    "                dados_brasil.append((ano, brasil_data))\n",
    "        \n",
    "        if len(dados_brasil) >= 2:\n",
    "            # Comparar primeiro e √∫ltimo ano dispon√≠vel\n",
    "            ano_inicial, df_inicial = dados_brasil[0]\n",
    "            ano_final, df_final = dados_brasil[-1]\n",
    "            \n",
    "            df_var = calcular_variacao_percentual(df_inicial, df_final, base)\n",
    "            colunas_var = [col for col in df_var.columns if col.endswith('_var_%')]\n",
    "            \n",
    "            # Top 3 maiores aumentos e diminui√ß√µes\n",
    "            variacoes = []\n",
    "            for coluna in colunas_var:\n",
    "                valor = df_var[coluna].iloc[0] if not df_var[coluna].empty else None\n",
    "                if valor is not None and not pd.isna(valor):\n",
    "                    variacoes.append((coluna.replace('_var_%', ''), valor))\n",
    "            \n",
    "            if variacoes:\n",
    "                variacoes.sort(key=lambda x: x[1], reverse=True)\n",
    "                \n",
    "                print(f\"üìà Maiores AUMENTOS ({ano_inicial}-{ano_final}):\")\n",
    "                for indicador, valor in variacoes[:3]:\n",
    "                    print(f\"   ‚Ä¢ {indicador}: +{valor:.1f}%\")\n",
    "                \n",
    "                print(f\"üìâ Maiores DIMINUI√á√ïES ({ano_inicial}-{ano_final}):\")\n",
    "                for indicador, valor in variacoes[-3:]:\n",
    "                    print(f\"   ‚Ä¢ {indicador}: {valor:.1f}%\")\n",
    "\n",
    "# Definir as compara√ß√µes desejadas\n",
    "comparacoes = {\n",
    "    (2018, 2019): 'comparacao_2018_vs_2019.xlsx',\n",
    "    (2022, 2023): 'comparacao_2022_vs_2023.xlsx',\n",
    "    (2018, 2023): 'comparacao_2018_vs_2023.xlsx'\n",
    "}\n",
    "\n",
    "# Executar as an√°lises\n",
    "print(\"Iniciando an√°lise comparativa temporal...\")\n",
    "gerar_comparacoes_temporais(dfs_por_ano, comparacoes)\n",
    "\n",
    "print(\"\\nGerando resumo estat√≠stico...\")\n",
    "gerar_resumo_estatistico(dfs_por_ano, comparacoes)\n",
    "\n",
    "# Executar an√°lises adicionais\n",
    "print(\"\\nGerando relat√≥rio por regi√£o...\")\n",
    "gerar_relatorio_por_regiao()\n",
    "\n",
    "print(\"\\nAnalisando tend√™ncias...\")\n",
    "analisar_tendencias()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXEMPLO: Maiores varia√ß√µes na base AFD (2018 vs 2023)\")\n",
    "print(\"=\"*50)\n",
    "try:\n",
    "    maiores_var = identificar_maiores_variacoes('output/comparacao_2018_vs_2023.xlsx', 'AFD', top_n=5)\n",
    "    print(maiores_var.to_string(index=False))\n",
    "except:\n",
    "    print(\"erro\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
